# ETL Test Suite

This directory contains a comprehensive test suite for validating ETL jobs using Databricks notebooks and automated data validation. The test suite works with lineage information generated by the [lineage-analyzer](https://github.com/harrydu-db/lineage-analyzer/) project to create comprehensive test metadata.

## Overview

The test suite consists of three main components that work together to provide comprehensive testing capabilities:

1. **`test-framework/`** - Core test framework for unit testing ETL jobs
2. **`test-builder/`** - Automated test data generation system
3. **`utils/`** - Utility scripts and tools for test data management and configuration

## Components

### Test Framework (`test-framework/`)

The test framework provides the core testing infrastructure for ETL jobs. It includes:

- **Automated Test Execution**: Runs ETL jobs and validates results
- **Data Preparation**: Creates tables and views from test data
- **Data Validation**: Compares expected results with actual table content
- **Databricks Integration**: Works with Databricks notebooks and clusters

For detailed documentation on the test framework, see [test-framework/README.md](test-framework/README.md).

### Test Builder (`test-builder/`)

The test builder provides automated test data generation capabilities that work in conjunction with the test framework:

- **Automated Test Data Generation**: Creates realistic test data using sophisticated filter resolution
- **Unit Test Environment Setup**: Prepares isolated test environments for ETL job execution
- **Test Data Management**: Generates both pre-test and post-test data for comprehensive validation

For detailed documentation on the test builder, see [test-builder/README.md](test-builder/README.md).

### Utilities (`utils/`)

The utilities provide essential tools for test data management, metadata generation, and configuration:

- **Test Data Management**: Upload and download test data to/from Databricks volumes
- **Metadata Generation**: Generate script metadata from lineage analysis
- **Configuration Management**: Handle YAML-based configuration files
- **Data Consolidation**: Consolidate and process script metadata

For detailed documentation on the utilities, see [utils/README.md](utils/README.md).

## Quick Start

### 1. Install Dependencies

```bash
# Install development dependencies
pip install -r requirements-dev.txt
```

### 2. Configure Databricks CLI

Set up your Databricks CLI profile:

```bash
# Configure your profile
databricks configure --profile your-profile-name
```

Update `tests/config.yml` with your profile and volume path. It is used by the utilities in tests/utils. 

### 3. Test Metadata

The test suite uses `script_metadata.json` files that are already provided in the `test-framework/` and `test-builder/` folders. These files contain dependency information for all ETL jobs and are generated from lineage analysis.

#### Pre-generated Metadata (Default)
The `script_metadata.json` files are already included and ready to use. They were created using:
1. **Lineage Analysis**: [lineage-analyzer](https://github.com/harrydu-db/lineage-analyzer/) analyzes ETL jobs in the source directory and generates lineage JSON files
2. **Metadata Generation**: `build_test_metadata.py` processes the lineage files to create comprehensive test metadata

#### Regenerate Metadata (Optional)
If you need to regenerate the metadata from updated lineage files:

```bash
# Generate script metadata from lineage files
python tests/utils/build_test_metadata.py
```

### 4. Test Data

The `test_data/` folder contains the test cases and definitions needed for running tests:

#### Test Data Structure

- **`test_data/`** - Root directory containing all test data
- **`test_data/table_definitions/`** - JSON schema definitions for all tables
- **`test_data/view_definitions/`** - SQL definitions for all views
- **`test_data/{script_name}/{test_name}/`** - Individual test case data
  - **`pre_test/`** - Input data (CSV files) before ETL job execution
  - **`post_test/`** - Expected output data (CSV files) after ETL job execution

#### Upload and Run Tests

```bash
# Upload all test data and definitions to volume
python tests/utils/upload_test_data.py
```

The `upload_test_data.py` utility uploads the test data and definitions from the `test_data/` folder to the volume attached to your Databricks workspace, making them available for the test framework. 

The `download_test_data.py` utility downloads the generated test data from the Databricks volume to your local machine for analysis or backup purposes. This is useful for reviewing test results, debugging, or creating backups of generated test data.

The utilities in the `utils/` folder use `tests/config.yml` for configuration settings.

## Test Workflow

The test suite supports two main workflows:

### 1. Automated Test Data Workflow (Recommended)
1. **Generate Test Data**: Use the test-builder to automatically generate realistic test data using pre-generated metadata
2. **Run Tests**: Execute ETL jobs using the test framework components
3. **Validate Results**: Compare generated post-test data with expected results

### 2. Manual Test Data Workflow (Traditional)
1. **Prepare Test Data**: Manually create CSV files with pre_test and post_test data
2. **Upload Data**: Use `utils/upload_test_data.py` to upload test data to Databricks volumes
3. **Run Tests**: Execute ETL jobs using the test framework components

## Complete ETL Testing Pipeline

The test-framework and test-builder are part of the Databricks Asset Bundle (DAB). When deployed using `databricks bundle deploy`, four jobs are created:

- **test-builder job**: Generates test data using the test-builder components
- **unittest job**: Executes ETL job tests using the test framework
- **unittest_parallel job**: Runs tests in parallel for better performance
- **unittest_debmo job**: Sample ETL job demonstrating framework capabilities

## Additional Resources

- **`utils/`** - Contains utilities for uploading test data, configuration management, and metadata building
- **`test_data/`** - Contains pre-defined test cases, table definitions, and view definitions
  - **`table_definitions/`** - JSON schema definitions for all tables
  - **`view_definitions/`** - SQL definitions for all views  
  - **`{script_name}/{test_name}/`** - Individual test case data with pre_test and post_test folders
- **`config.yml`** - Configuration file for Databricks workspace settings
- **`script_metadata.json`** - Pre-generated metadata files containing ETL job dependencies (in both `test-framework/` and `test-builder/` folders)
- **`old/lineage/`** - Lineage JSON files generated by the [lineage-analyzer](https://github.com/harrydu-db/lineage-analyzer/) project

## Documentation

- [Test Framework Documentation](test-framework/README.md) - Detailed documentation for the core test framework
- [Test Builder Documentation](test-builder/README.md) - Detailed documentation for automated test data generation
- [Utilities Documentation](utils/README.md) - Documentation for utility scripts and tools

## Support

For questions or issues with the test suite, please refer to the individual component documentation or contact the development team.