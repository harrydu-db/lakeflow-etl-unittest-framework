# ETL Test Framework Utilities

This directory contains essential utility scripts and tools that support the LakeFlow ETL Unit Test Framework. These utilities handle test data management, metadata generation, configuration management, and data consolidation.

## Overview

The utilities provide comprehensive support for the test framework by managing:
- **Test Data Management**: Upload and download test data to/from Databricks volumes
- **Metadata Generation**: Generate script metadata from lineage analysis
- **Configuration Management**: Handle YAML-based configuration files
- **Data Consolidation**: Consolidate and process script metadata

## Utility Scripts

### 1. Test Data Management

#### `upload_test_data.py`
Uploads test data and definitions from the local `test_data/` folder to Databricks volumes.

**Features:**
- Uploads entire test data folder or specific subdirectories/files
- Always overwrites destination (deletes first, then uploads)
- Supports individual files or entire directories
- Uses Databricks CLI for reliable data transfer
- Configurable via `tests/config.yml`

**Usage:**
```bash
# Upload entire test_data folder
python tests/utils/upload_test_data.py

# Upload specific folder
python tests/utils/upload_test_data.py sample/test_basic

# Upload specific file
python tests/utils/upload_test_data.py table_definitions/file.json
```

**Configuration:**
- Uses `tests/config.yml` for Databricks profile and volume settings
- Supports environment variable substitution
- Configurable source and destination paths

#### `download_test_data.py`
Downloads test data and definitions from Databricks volumes to local folders.

**Features:**
- Downloads entire volumes or specific subdirectories/files
- Preserves local directory structure
- Supports individual files or entire directories
- Uses Databricks CLI for reliable data transfer
- Configurable via `tests/config.yml`

**Usage:**
```bash
# Download entire source volume
python tests/utils/download_test_data.py

# Download specific folder
python tests/utils/download_test_data.py sample/test_basic

# Download specific file
python tests/utils/download_test_data.py table_definitions/file.json
```

**Configuration:**
- Uses `tests/config.yml` for Databricks profile and volume settings
- Supports environment variable substitution
- Configurable source and destination paths

### 2. Metadata Generation

#### `build_test_metadata.py`
Generates comprehensive script metadata from lineage analysis for automated test data generation.

**Features:**
- Analyzes lineage JSON files to build dependency graphs
- Identifies non-volatile tables and their relationships
- Determines view/table dependencies using topological sorting
- Generates script metadata for test framework and test builder
- Handles complex dependency resolution and volatility filtering

**Usage:**
```bash
# Generate script metadata from lineage files
python tests/utils/build_test_metadata.py
```

**Input:**
- Lineage JSON files from `old/lineage/` directory
- Generated by [lineage-analyzer](https://github.com/harrydu-db/lineage-analyzer/) project

**Output:**
- `script_metadata.json` files for test-framework and test-builder
- Comprehensive metadata including tables, views, update_tables, and reference_tables

**Key Capabilities:**
- **Dependency Analysis**: Builds complete dependency graphs for tables and views
- **Volatility Filtering**: Excludes temporary and staging tables from test data
- **Topological Sorting**: Ensures views are created in proper dependency order
- **Update Detection**: Identifies which tables are modified vs referenced

### 3. Data Consolidation

#### `consolidate_script_metadata.py`
Consolidates script metadata into a unified structure providing a comprehensive view of all tables, views, and relationships.

**Features:**
- Consolidates multiple script metadata files into a single structure
- Provides unified view of all tables, views, and relationships
- Maintains topological ordering for views
- Deduplicates entries across all scripts
- Generates reference-only table analysis

**Usage:**
```bash
python tests/utils/consolidate_script_metadata.py <input_file_path>
```

**Example:**
```bash
python tests/utils/consolidate_script_metadata.py script_metadata.json
```

**Output Structure:**
- **`tables`**: All unique tables from all scripts, sorted alphabetically
- **`views`**: All unique views from all scripts, maintaining topological order
- **`reference_tables`**: All unique reference tables, sorted alphabetically
- **`updated_tables`**: All unique updated tables, sorted alphabetically
- **`reference_only_tables`**: Reference tables that are not updated tables

### 4. Configuration Management

#### `config.py`
Provides centralized configuration management for the test framework using YAML files and environment variables.

**Features:**
- Loads configuration from YAML files
- Supports environment variable substitution
- Provides easy access to configuration values
- Handles missing configuration files gracefully
- Supports nested configuration structures

**Usage:**
```python
from utils.config import Config

# Initialize configuration
config = Config("tests/config.yml")

# Access configuration values
profile = config.get("databricks.profile")
volume_path = config.get("databricks.volume_path")
```

**Configuration File Structure:**
```yaml
databricks:
  profile: "your-profile-name"
  volume_path: "/Volumes/your-catalog/default/test_data"
  workspace_url: "https://your-workspace.cloud.databricks.com"

test_framework:
  unittest_catalog: "unittest_catalog"
  max_workers: 4
```

## Configuration

### `tests/config.yml`
The main configuration file used by all utilities. Contains:

- **Databricks Settings**: Profile, workspace URL, volume paths
- **Test Framework Settings**: Catalog names, worker counts, timeouts
- **Environment Variables**: Support for environment variable substitution

### Environment Variables
The utilities support environment variable substitution in configuration files:

```yaml
databricks:
  profile: "${DATABRICKS_PROFILE}"
  volume_path: "/Volumes/${CATALOG}/default/test_data"
```

## Dependencies

### Python Requirements
- **Python 3.8+**: Required for all utilities
- **PyYAML**: For configuration file handling
- **python-dotenv**: For environment variable loading

### External Dependencies
- **Databricks CLI**: Required for upload/download operations
- **Databricks Workspace Access**: Required for volume operations
- **Lineage Analyzer**: Required for metadata generation

## Integration with Test Framework

### Test Framework Integration
- **Metadata Loading**: `config.py` provides configuration access for test framework
- **Test Data Access**: Upload/download utilities manage test data for framework execution
- **Script Metadata**: Generated metadata files are used by both test-framework and test-builder

### Test Builder Integration
- **Script Metadata**: Generated metadata files provide dependency information for test data generation
- **Configuration Access**: `config.py` provides configuration for test builder execution
- **Test Data Management**: Upload/download utilities handle test data for test builder

## Error Handling

### Common Error Scenarios
- **Missing Configuration**: Graceful handling of missing config files with helpful error messages
- **Databricks CLI Errors**: Comprehensive error reporting for CLI command failures
- **File Not Found**: Clear error messages for missing files or directories
- **Permission Issues**: Detailed error reporting for access permission problems

### Error Recovery
- **Retry Logic**: Built-in retry mechanisms for transient failures
- **Partial Success**: Continues processing even when individual operations fail
- **Detailed Logging**: Comprehensive error messages with context information

## Best Practices

### Configuration Management
- Use environment variables for sensitive information (API keys, passwords)
- Keep configuration files in version control with placeholder values
- Use descriptive configuration keys with proper nesting

### Test Data Management
- Always backup test data before major operations
- Use specific paths when uploading/downloading to avoid overwriting important data
- Verify data integrity after upload/download operations

### Metadata Generation
- Regenerate metadata after significant changes to ETL scripts
- Validate generated metadata before using in test framework
- Keep lineage files up to date for accurate dependency analysis

## Troubleshooting

### Common Issues

#### Databricks CLI Not Found
```bash
# Install Databricks CLI
pip install databricks-cli

# Configure CLI
databricks configure --profile your-profile-name
```

#### Permission Denied Errors
- Verify Databricks workspace access
- Check volume permissions
- Ensure proper authentication

#### Configuration File Not Found
- Verify `tests/config.yml` exists
- Check file path in utility calls
- Ensure proper working directory

#### Upload/Download Failures
- Check Databricks CLI configuration
- Verify volume paths and permissions
- Check network connectivity

### Debug Mode
Most utilities support verbose logging. Use `--verbose` or `--debug` flags for detailed output:

```bash
python tests/utils/upload_test_data.py --verbose
python tests/utils/download_test_data.py --debug
```

## Support

For questions or issues with the utilities, please refer to:
- Individual script documentation (inline comments and docstrings)
- Test framework documentation
- Databricks CLI documentation
- [Lineage Analyzer documentation](https://github.com/harrydu-db/lineage-analyzer/)

## Related Documentation

- [Test Framework Documentation](../test-framework/README.md)
- [Test Builder Documentation](../test-builder/README.md)
- [Test Suite Overview](../README.md)
- [Consolidate Script Metadata Documentation](README_consolidate.md)
